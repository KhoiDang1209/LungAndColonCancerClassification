{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zxn1HKMpRTSK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models"
      ],
      "metadata": {
        "id": "ZvPY7dVfVD4t"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "\n",
        "# Then move kaggle.json into the folder where the API expects to find it.\n",
        "!mkdir -p ~/.kaggle/ && mv kaggle.json ~/.kaggle/ && chmod 600 ~/.kaggle/kaggle.json\n",
        "# --- The change is in the next line ---\n",
        "!cp ~/.kaggle/kaggle.json ~/.config/kaggle/ # Copy the file to the path expected by the API"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "FQOFv5ab0oh9",
        "outputId": "88e65ca9-2676-4707-d51e-68870b8144ec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-22b1cbb1-53b1-4670-80e2-1c8843749d24\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-22b1cbb1-53b1-4670-80e2-1c8843749d24\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n",
            "User uploaded file \"kaggle (1).json\" with length 68 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import kagglehub\n",
        "import kaggle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "5EPSKzqLV_Gw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle.api.authenticate()\n",
        "kaggle.api.dataset_download_files('andrewmvd/lung-and-colon-cancer-histopathological-images', path='./', unzip=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdFcu1JEwAZU",
        "outputId": "7e90181e-ea8b-4cb6-ddda-09cac7268369"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/andrewmvd/lung-and-colon-cancer-histopathological-images\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyLungCancerDataset(Dataset):\n",
        "    def __init__(self, data_folder, csv_path):\n",
        "        self.data_folder = data_folder\n",
        "        self.labels_df = pd.read_csv(csv_path)\n",
        "\n",
        "        # Recursively get all image paths inside data_folder\n",
        "        self.image_paths = []\n",
        "        for root, _, files in os.walk(data_folder):\n",
        "            for file in files:\n",
        "                if file.endswith(\".jpeg\"):\n",
        "                    self.image_paths.append(os.path.join(root, file))\n",
        "\n",
        "        # Extract only images present in CSV file\n",
        "        self.image_paths = [path for path in self.image_paths if os.path.basename(path) in self.labels_df['file_name'].tolist()]\n",
        "\n",
        "        # Define transformations\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "        ])\n",
        "\n",
        "        # Map labels to integers\n",
        "        self.label_map = {\n",
        "            'lung_n': 0,\n",
        "            'lung_aca': 1,\n",
        "            'lung_scc': 2,\n",
        "            'colon_n': 3,\n",
        "            'colon_aca': 4\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img_path = self.image_paths[index]\n",
        "        image = Image.open(img_path).convert(\"RGB\")  # Ensure RGB format\n",
        "        transformed_image = self.transform(image)\n",
        "\n",
        "        # Get label from CSV\n",
        "        image_name = os.path.basename(img_path)\n",
        "        label_str = self.labels_df[self.labels_df.file_name == image_name]['label'].values[0]\n",
        "        label = self.label_map.get(label_str, -1)  # Assign -1 if label is missing\n",
        "\n",
        "        return transformed_image, label"
      ],
      "metadata": {
        "id": "TQWTA_ycctEi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset=MyLungCancerDataset(\n",
        "    data_folder='/content/lung_colon_image_set',\n",
        "    csv_path='/content/drive/MyDrive/lung_cancer/lung_and_colon_image_labels.csv'\n",
        ")"
      ],
      "metadata": {
        "id": "JGv2EevN3Ijv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aau4QRT03zHS",
        "outputId": "b1afeba6-cc46-46ad-aec8-e62dc6cc1ea9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3-ZBqYnU9BE",
        "outputId": "5a568bc1-35a2-4876-aae0-ddf671a3b396"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.0510,  0.4745,  0.6314,  ...,  0.5843,  0.6471,  0.8431],\n",
              "          [-0.1059,  0.3804,  0.4902,  ...,  0.6706,  0.6549,  0.8745],\n",
              "          [-0.0118,  0.3804,  0.4353,  ...,  0.6549,  0.6863,  0.9137],\n",
              "          ...,\n",
              "          [ 0.8039,  0.8118,  0.8196,  ...,  0.8118,  0.7098,  0.7176],\n",
              "          [ 0.8667,  0.8824,  0.8824,  ...,  0.8118,  0.7490,  0.7020],\n",
              "          [ 0.8980,  0.9373,  0.9529,  ...,  0.8431,  0.8196,  0.7490]],\n",
              " \n",
              "         [[-0.2000,  0.2078,  0.3569,  ...,  0.1922,  0.3333,  0.6549],\n",
              "          [-0.3882,  0.0980,  0.2078,  ...,  0.2863,  0.3490,  0.6706],\n",
              "          [-0.3098,  0.1059,  0.1686,  ...,  0.3098,  0.3882,  0.6863],\n",
              "          ...,\n",
              "          [ 0.4667,  0.5059,  0.5529,  ...,  0.4824,  0.3333,  0.2941],\n",
              "          [ 0.5451,  0.6000,  0.6471,  ...,  0.5451,  0.4353,  0.3098],\n",
              "          [ 0.5922,  0.6784,  0.7333,  ...,  0.6078,  0.5451,  0.3882]],\n",
              " \n",
              "         [[ 0.5373,  0.7961,  0.8275,  ...,  0.8039,  0.7569,  0.8510],\n",
              "          [ 0.4431,  0.8118,  0.8275,  ...,  0.8902,  0.7804,  0.8902],\n",
              "          [ 0.4902,  0.8196,  0.8275,  ...,  0.8902,  0.8275,  0.9451],\n",
              "          ...,\n",
              "          [ 0.8745,  0.8510,  0.9059,  ...,  0.9137,  0.8667,  0.8824],\n",
              "          [ 0.8902,  0.8667,  0.9216,  ...,  0.8588,  0.8431,  0.8118],\n",
              "          [ 0.8745,  0.8824,  0.9451,  ...,  0.8510,  0.8588,  0.8118]]]),\n",
              " 3)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    num_workers=2\n",
        ")\n",
        "train_dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eW1uqF3BW6S2",
        "outputId": "ee37a43c-4794-40b5-c380-0ac7db206a98"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x79accc757850>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MyCNN(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyCNN,self).__init__()\n",
        "\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "    self.linear1 = nn.Linear(in_features=32 * 56 * 56, out_features=256)\n",
        "    self.relu3 = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(in_features=256, out_features=5)\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x=self.conv1(x)\n",
        "    x=self.relu1(x)\n",
        "    x=self.pool1(x)\n",
        "\n",
        "    x=self.conv2(x)\n",
        "    x=self.relu2(x)\n",
        "    x=self.pool2(x)\n",
        "\n",
        "    x=x.view(-1,32*56*56)\n",
        "    x=self.linear1(x)\n",
        "    x=self.relu3(x)\n",
        "    x=self.linear2(x)\n",
        "    x=self.softmax(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "4-LmrNgjW-1b"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyCNN()\n",
        "model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61WFM1ySsigH",
        "outputId": "a33acf12-0b63-4bef-e4fa-cdf6143ce43b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MyCNN(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu1): ReLU()\n",
              "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (relu2): ReLU()\n",
              "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (linear1): Linear(in_features=100352, out_features=256, bias=True)\n",
              "  (relu3): ReLU()\n",
              "  (linear2): Linear(in_features=256, out_features=5, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MyCNN().to(device)"
      ],
      "metadata": {
        "id": "xH5Ix2u2mV0G"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_func = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
      ],
      "metadata": {
        "id": "B0ecUnfUsncP"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer = SummaryWriter(log_dir='train_logs_1')\n",
        "writer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sMxU_PKWsu1I",
        "outputId": "52c9eb18-e886-4f7f-8cf6-e381766e6493"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.tensorboard.writer.SummaryWriter at 0x79accc2bca10>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epoch = 10"
      ],
      "metadata": {
        "id": "gqtRN4XPs0Jn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ckpt_folder='ckpt'\n",
        "os.makedirs(ckpt_folder, exist_ok=True)"
      ],
      "metadata": {
        "id": "m4nKBsrwzBAJ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epoch):\n",
        "  model.train()\n",
        "  for iteration_, (images, labels) in enumerate(tqdm(train_dataloader, total=len(train_dataloader))):\n",
        "    images, labels = images.to(device), labels.to(device)  # Move to GPU\n",
        "    optimizer.zero_grad()\n",
        "    pred = model(images)\n",
        "    loss_value = loss_func(pred, labels)\n",
        "    loss_value.backward()\n",
        "    optimizer.step()\n",
        "    global_iteration = epoch * len(train_dataloader) + iteration_\n",
        "    writer.add_scalar('train_loss_iter', loss_value, global_iteration)\n",
        "\n",
        "  print(f'Epoch={epoch}', f'Training loss={loss_value.item()}')\n",
        "\n",
        "  writer.add_scalar('train_loss_epoch', loss_value, epoch)\n",
        "\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "\n",
        "    loss_sum=0\n",
        "    pred_list, label_list = [], []\n",
        "\n",
        "    for images, labels in tqdm(train_dataloader, total=len(train_dataloader)):\n",
        "      images, labels = images.to(device), labels.to(device)  # Move to GPU\n",
        "      pred = model(images)\n",
        "      loss_value = loss_func(pred, labels)\n",
        "      loss_sum += loss_value.item()\n",
        "      pred_list.extend(torch.argmax(pred, dim=1).tolist())\n",
        "      label_list.extend(labels.tolist())\n",
        "\n",
        "    print(f'Test loss={loss_sum/len(train_dataloader)}')\n",
        "    writer.add_scalar('test_loss_epoch', loss_value,epoch)\n",
        "\n",
        "    # Ensure pred_list and label_list are tensors before concatenation\n",
        "    final_pred = torch.tensor(pred_list)\n",
        "    final_label = torch.tensor(label_list)\n",
        "\n",
        "    # If you're performing classification, you likely want to compare predictions directly\n",
        "    epoch_accuracy = accuracy_score(final_label, final_pred)\n",
        "    epoch_precision = precision_score(final_label, final_pred, average='weighted') # Assuming you need weighted average for multi-class\n",
        "    epoch_recall = recall_score(final_label, final_pred, average='weighted')\n",
        "    epoch_f1 = f1_score(final_label, final_pred, average='weighted')\n",
        "\n",
        "    writer.add_scalar('test_accuracy_epoch', epoch_accuracy, epoch)\n",
        "    writer.add_scalar('test_precision_epoch', epoch_precision, epoch)\n",
        "    writer.add_scalar('test_recall_epoch', epoch_recall, epoch)\n",
        "    writer.add_scalar('test_f1_epoch', epoch_f1, epoch)\n",
        "\n",
        "    print(classification_report(final_label, final_pred))\n",
        "\n",
        "    torch.save(model.state_dict(), os.path.join('ckpt', f'ckpt_{epoch}.pth'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8V7iKNVs11x",
        "outputId": "122771ff-2f5a-41d6-d57b-08114cd8666e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:16<00:00,  3.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=0 Training loss=1.035280466079712\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:12<00:00,  3.10it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.1268312078912546\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.94      0.95      5000\n",
            "           1       0.78      0.78      0.78      5000\n",
            "           2       0.83      0.85      0.84      5000\n",
            "           3       0.64      0.96      0.76      5000\n",
            "           4       0.87      0.43      0.58      5000\n",
            "\n",
            "    accuracy                           0.79     25000\n",
            "   macro avg       0.82      0.79      0.78     25000\n",
            "weighted avg       0.82      0.79      0.78     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:17<00:00,  3.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=1 Training loss=1.0344170331954956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:04<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.0856393282218357\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.94      0.96      5000\n",
            "           1       0.81      0.83      0.82      5000\n",
            "           2       0.88      0.85      0.87      5000\n",
            "           3       0.68      0.98      0.80      5000\n",
            "           4       0.88      0.54      0.67      5000\n",
            "\n",
            "    accuracy                           0.83     25000\n",
            "   macro avg       0.85      0.83      0.83     25000\n",
            "weighted avg       0.85      0.83      0.83     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:14<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=2 Training loss=1.1701042652130127\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:01<00:00,  3.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.082475118548669\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.94      0.95      5000\n",
            "           1       0.81      0.85      0.83      5000\n",
            "           2       0.89      0.86      0.87      5000\n",
            "           3       0.79      0.68      0.73      5000\n",
            "           4       0.70      0.81      0.75      5000\n",
            "\n",
            "    accuracy                           0.83     25000\n",
            "   macro avg       0.83      0.83      0.83     25000\n",
            "weighted avg       0.83      0.83      0.83     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:14<00:00,  3.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=3 Training loss=1.031019926071167\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:04<00:00,  3.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.0637927675033774\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.94      0.96      5000\n",
            "           1       0.83      0.84      0.83      5000\n",
            "           2       0.88      0.88      0.88      5000\n",
            "           3       0.72      0.97      0.82      5000\n",
            "           4       0.90      0.60      0.72      5000\n",
            "\n",
            "    accuracy                           0.85     25000\n",
            "   macro avg       0.86      0.85      0.84     25000\n",
            "weighted avg       0.86      0.85      0.84     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:13<00:00,  3.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=4 Training loss=1.0901637077331543\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:11<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.0522458193552158\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97      5000\n",
            "           1       0.79      0.90      0.84      5000\n",
            "           2       0.91      0.84      0.88      5000\n",
            "           3       0.80      0.85      0.82      5000\n",
            "           4       0.82      0.76      0.79      5000\n",
            "\n",
            "    accuracy                           0.86     25000\n",
            "   macro avg       0.86      0.86      0.86     25000\n",
            "weighted avg       0.86      0.86      0.86     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:07<00:00,  3.16it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=5 Training loss=1.1607240438461304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:05<00:00,  3.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.0360201068241577\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97      5000\n",
            "           1       0.87      0.83      0.85      5000\n",
            "           2       0.86      0.93      0.89      5000\n",
            "           3       0.76      0.96      0.85      5000\n",
            "           4       0.92      0.69      0.79      5000\n",
            "\n",
            "    accuracy                           0.87     25000\n",
            "   macro avg       0.88      0.87      0.87     25000\n",
            "weighted avg       0.88      0.87      0.87     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:19<00:00,  3.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=6 Training loss=1.0223369598388672\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:05<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.028660893973792\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97      5000\n",
            "           1       0.87      0.82      0.85      5000\n",
            "           2       0.85      0.93      0.89      5000\n",
            "           3       0.78      0.95      0.86      5000\n",
            "           4       0.92      0.73      0.81      5000\n",
            "\n",
            "    accuracy                           0.88     25000\n",
            "   macro avg       0.88      0.88      0.87     25000\n",
            "weighted avg       0.88      0.88      0.87     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:17<00:00,  3.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=7 Training loss=1.1175885200500488\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:09<00:00,  3.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.0161886387468908\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97      5000\n",
            "           1       0.87      0.86      0.87      5000\n",
            "           2       0.88      0.93      0.90      5000\n",
            "           3       0.81      0.94      0.87      5000\n",
            "           4       0.91      0.78      0.84      5000\n",
            "\n",
            "    accuracy                           0.89     25000\n",
            "   macro avg       0.89      0.89      0.89     25000\n",
            "weighted avg       0.89      0.89      0.89     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:11<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=8 Training loss=1.0322984457015991\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:11<00:00,  3.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.0116104774767785\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97      5000\n",
            "           1       0.83      0.93      0.88      5000\n",
            "           2       0.94      0.87      0.90      5000\n",
            "           3       0.83      0.94      0.88      5000\n",
            "           4       0.92      0.81      0.86      5000\n",
            "\n",
            "    accuracy                           0.90     25000\n",
            "   macro avg       0.90      0.90      0.90     25000\n",
            "weighted avg       0.90      0.90      0.90     25000\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:15<00:00,  3.06it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch=9 Training loss=1.1457183361053467\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 782/782 [04:06<00:00,  3.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss=1.0054890516468935\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.94      0.97      5000\n",
            "           1       0.88      0.89      0.88      5000\n",
            "           2       0.90      0.93      0.92      5000\n",
            "           3       0.88      0.86      0.87      5000\n",
            "           4       0.86      0.89      0.87      5000\n",
            "\n",
            "    accuracy                           0.90     25000\n",
            "   macro avg       0.90      0.90      0.90     25000\n",
            "weighted avg       0.90      0.90      0.90     25000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}